---
title: "Tidying Public Ecological Data"
author: "Matthew Ross"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  ioslides_presentation:
    incremental: true
    widescreen: true
    smaller: true
    transition: faster
editor_options: 
  chunk_output_type: console
---

<style>
slides > slide {
  overflow-x: auto !important;
  overflow-y: auto !important;
}
</style>


# Why public datasets?

- Cheaper way to test initial ideas
- Expand local study or experiment to other ecosystems
- Regional or global model validation
- More!


## Tidying Messy Public data

- Working with public data can be messy
- R has a great universe of packages to tidy messy data
- Most folks use [tidyverse](https://www.tidyverse.org/)
- The book [R for Data Science](https://r4ds.had.co.nz/) is a great resource

## Goals for this lab

- Introduce key packages for downloading and working with public data
- Show examples of the power of the `tidyverse`
- Point to resources for learning how to use these tools

## Key packages 

- tidyverse - meta-package of packages with dplyr, tibble, readr, and others
- dataRetrieval - package to download water quality data
- sf - geospatial package
- mapview - make interactive maps
- broom - tidy up modelling data
- knitr - make pretty tables
- kableExtra - make scrollable pretty tables
- ggthemes - make prettier plots
- tidyr - tidy data into long or wide formats. 


```{r setup, warnings='hide',message=FALSE,include=F}
library(tidyverse) # Package with dplyr, tibble, readr, and others to help clean coding
library(dataRetrieval) # Package to download data. 
library(sf) #Geospatial package to plot and explore data
library(mapview) #Simple interface to leaflet interactive maps
library(broom) #Simplifies model outputs
library(knitr) #Makes nice tables
library(kableExtra) #Makes even nicer tables
library(lubridate) #Makes working with dates easier
library(ggthemes) #Makes plots prettier
library(tidyr) #Makes multiple simultaneous models easier


#Move the directory to the top folder level
knitr::opts_knit$set(root.dir='..')
knitr::opts_chunk$set(cache=T,fig.width=8,height=6)

```

# Downloading data.

- Working with water quality data in the Colorado Basin
- dataRetrieval makes it easy to download data
- We'll focus on cation and anion data (because that's what I'm interested in)
- Follow dataRetrieval package help to make download commands


## Download prep

```{r download prep}
#First we'll make a tibble (a tidyverse table) with Site IDs.
#Gen erally these are increasingly downstream of the CO headwaters near Grand Lake. 
colorado <- tibble(sites=c('USGS-09034500','USGS-09069000','USGS-09071100',
                           'USGS-09085000','USGS-09095500','USGS-09152500',
                           'USGS-09180000','USGS-09180500','USGS-09380000'),
                   basin=c('colorado1','eagle','colorado2',
                           'roaring','colorado3','gunnison',
                           'dolores','colorado4','colorado5'))

#Now we need to setup a series of rules for downloading data from the Water Quality Portal. 
#We'll focus on cation and anion data from 1950-present. 
#Each cation has a name that we might typically use like calcium or sulfate, 
#but the name may be different in the water quality portal, 
#so we have to check this website https://www.waterqualitydata.us/Codes/Characteristicname?mimeType=xml 
#to get our names correct. 

paramater.names <- c('ca','mg','na','k','so4','cl','hco3')

ca <- c('Calcium')
mg <- c('Magnesium')
na <- 'Sodium'
k <- 'Potassium'
so4 <- c('Sulfate','Sulfate as SO4','Sulfur Sulfate','Total Sulfate')
cl <- 'Chloride'
hco3 <- c('Alkalinity, bicarbonate','Bicarbonate')

#Compile all these names into a single list
parameters <- list(ca,mg,na,k,so4,cl,hco3)
#Name each cation or anion in the list
names(parameters) <- paramater.names
#Notice that we aren't downloading any nutrients (P or N)
#because they are much messier (100s of different ways to
#measure and report concentration data) than other cation anion data. 

#Start dates
start <- '1950-10-01'
end <- '2018-09-30'

#Sample media (no sediment samples)
sampleMedia = 'Water'

#Comple all this information into a list with arguments
site.args <- list(siteid=colorado$sites,
                  sampleMedia=sampleMedia,
                  startDateLo=start,
                  startDateHi=end,
                  characteristicName=NA) #We'll fill this in later in a loop



```

## Concentration data download

- Using the above parameters we can download all the data for these sites in ~10 minutes
- Pipes `%>%` move the results from one command to the next.
- Pipes clean up code and prevent redundant object creation


```{r concentration download, eval=F}
conc.list <- list() #Empty list to hold each data download


#We'll loop over each anion or cation and download all data at our sites for that constituent
for(i in 1:length(parameters)){
  #We need to rename the characteristicName (constituent) each time we go through the loop
  site.args$characteristicName<-parameters[[i]]
  
  #readWQPdata takes in our site.args list and downloads the data according to those rules 
  # time, constituent, site, etc...
  
  # Don't forget about pipes "%>%"! Pipes pass forward the results of a previous command, so that 
  #You don't have to constantly rename variables. I love them. 
  
  conc.list[[i]] <- readWQPdata(site.args) %>%
    mutate(parameter=names(parameters)[i]) #Mutate just adds a new column to the data frame
  
  #Pipes make the above command simple and succinct versus something more complicated like:
  
  ## conc.list[[i]] <- readWQPdata(site.args) 
  ## conc.list[[i]]$parameter <- names(parameters)[i]
}

#bind all this data together into a single data frame
conc.long <- map_dfr(conc.list,rbind)


```

## Site info download

- We can use `dplyr::select` to rename and select only useful columns
- `dplyr::distinct` keeps only the first result of duplicates 

```{r site info download, eval=F}
#In addition to concentration informatino, we probably want to know some things about the sites
#dplyr::select can help us only keep site information that is useful. 

site.info <- whatWQPsites(siteid=colorado$sites) %>%
  dplyr::select(SiteID=MonitoringLocationIdentifier,
                  name=MonitoringLocationName,
                  area=DrainageAreaMeasure.MeasureValue,
                  area.units=DrainageAreaMeasure.MeasureUnitCode,
                  lat=LatitudeMeasure,
                  long=LongitudeMeasure) %>%
  distinct() #Distinct just keeps the first of any duplicates. 

#The above function and map are equivalent to something more like this: 
  # site.list <- list()
  # for(i in 1:length(colorado$sites)){
  #   site.list[[i]] <- whatWQPsites(siteid=colorado$sites[i]) %>%
  #       dplyr::select(SiteID=MonitoringLocationIdentifier,
  #                   name=MonitoringLocationName,
  #                   area=DrainageAreaMeasure.MeasureValue,
  #                   area.units=DrainageAreaMeasure.MeasureUnitCode,
  #                   lat=LatitudeMeasure,
  #                   long=LongitudeMeasure)
  # }
  # 
  # site.info <- do.call(site.list,'rbind')

```

## Data save

-`save` can save files in the ultra-efficient `Rds` or `.RData` formats

```{r save downloads,eval=F}
#So that we don't have to download that data everytime
save(site.info,conc.long,file='WaterQuality_Tidying/wqp_co_data.RData')

```

## Data Load

- `load` can load these datasets efficiently and quickly
```{r data readin}
load('WaterQuality_Tidying/wqp_co_data.RData')

```


# Data tidying

- The data we just downloaded is messy
- Now we need to tidy it up to make it ready for analyses. 

## Look at the data you downloaded.

- Whenever you start with new data, look at the structure first
- you can use the command `head` or `str` to examine data structure
- Site.info data looks pretty clean and clear. 

```{r site info}

head(site.info)
```


## Concentration data

- Unlike the site data, this data is messy and verbose

```{r conc data}
head(conc.long) %>%
  kable(.,'html') %>%
  kable_styling() %>%
  scroll_box(width='800px',height='300px')
  
```


## Initial cleaning up

- We can use `dplyr` commands to help clean this data
- `select` can select and rename columns
- `mutate` can add or alter the data in a column
- `filter` can subset data
- `trimws` trims whitespace around character strings


```{r tidying up concentration}
#This code mostly just grabs and renames the most important data columns
conc.clean <-  conc.long %>%
                  dplyr::select(date=ActivityStartDate,
                         parameter=CharacteristicName,
                         units=ResultMeasure.MeasureUnitCode,
                         SiteID=MonitoringLocationIdentifier,
                         org=OrganizationFormalName,
                         org_id=OrganizationIdentifier,
                         time=ActivityStartTime.Time,
                         value=ResultMeasureValue,
                         sample_method=SampleCollectionMethod.MethodName,
                         analytical_method=ResultAnalyticalMethod.MethodName,
                         particle_size=ResultParticleSizeBasisText,
                         date_time=ActivityStartDateTime,
                         media=ActivityMediaName,
                         sample_depth=ActivityDepthHeightMeasure.MeasureValue,
                         sample_depth_unit=ActivityDepthHeightMeasure.MeasureUnitCode,
                         fraction=ResultSampleFractionText,
                         status=ResultStatusIdentifier) %>%
  #Remove trailing white space in labels
  mutate(units = trimws(units)) %>%
  #Keep only samples that are water samples
  filter(media=='Water') #Some of these snuck through!

```

## Tidier WQ data

- With some initial tidying the data is a lot easier to understand
- This data is in the long format: Each observation has a row while each variable has a column
- Long format data is the backbone of the tidyverse and makes all future operations easier


```{r examine tidier data}
head(conc.long) %>%
  kable(.,'html') %>%
  kable_styling() %>%
  scroll_box(width='800px',height='300px')
```

## Final tidy dataset

- Let's assume that all analytical methods used to collect these data are exchangeable
- If that is true then all we need to do is make sure the units are sensible

```{r unit check}
table(conc.clean$units)
```

- Wow! Almost all the data is in mg/L. That makes our job really easy. 

- Use `dplyr::filter` to remove those non-harmonized units
- Use `lubridate::ymd` call to turn date data into a date object. 

```{r tidy}
conc.tidy <- conc.clean %>% 
  filter(units != 'tons/day') %>%
  mutate(date=ymd(date)) %>%
  select(date,
         parameter,
         SiteID,
         conc=value)


```

## Daily data

- Now we have a manageable data frame
- But how do we want to organize the data?
- With lots of data, let's look at data as a daily average.
- The `dplyr::group_by` and  `summarize` commands make this really easy

```{r daily}


#The amazing group_by function groups all the data so that the summary
#only applies to each subgroup (site, date, and parameter combination).
#So in the end you get a daily average concentratino for each site and parameter type. 
conc.daily <- conc.tidy %>%
  group_by(date,parameter,SiteID) %>% 
  summarize(conc=mean(conc,na.rm=T))

```

Taking daily averages looks like it did eliminate `r nrow(conc.tidy) - nrow(conc.daily)` observations, meaning these site date combinations had multiple observations on the same day. 


# Analyzing data


## Map

- We can use the `sf` package to project the site information data
- This data object is called a `simple feature (sf)`. 
- The function `st_as_sf` converts the long (x) and lat (y) coordinates into a projected point feature with the EPSG code 4326 (WGS 84).
-We can then use the `mapview` package and function to look at where these sites are. 

```{r}
#convert site info as an sf object
site.sf <- site.info %>%
  st_as_sf(.,coords=c('long','lat'), crs=4326)


mapview(site.sf)

```


## Concentration data



## Calcium only


```{r daily plot}
conc.daily %>%
  filter(parameter == 'Calcium') %>%
  ggplot(.,aes(x=date,y=conc)) + 
  geom_point() + 
  facet_wrap(~SiteID)
  
```



## Annual summaries of full sites

- Let's shrink the dataset to only look at annual change. 
- Again we can use the powers of `group_by` and `summarize` to do this

```{r annual only}
too.few.years <- c('USGS-09034500','USGS-0907110','USGS-0908500')

conc.annual <- conc.daily %>%
  filter(!SiteID %in% too.few.years) %>% #! means opposite of, so we want all the sites not in the too.few years vector. 
  mutate(year=year(date)) %>%
  group_by(SiteID,year,parameter) %>%
  summarize(annual_mean=mean(conc,na.rm=T),
            annual_var=var(conc,na.rm=T))
  

```

## Plot of all the annual data.

```{r ugly}
conc.annual %>%
  ggplot(.,aes(x=year,y=annual_mean,color=SiteID)) + 
  geom_point() + 
  facet_wrap(~parameter,scales='free')
```

That plot is... ugly! Maybe we can make something prettier


## Prettier annual plot. 

- Join the data to basin names so data is easier to interpret
- Use the command `left_join` do this
- Use the command `rename` to make sure joining columns have the same name


```{r pretty,fig.width=9,fig.height=7}
conc.annual %>%
  left_join(colorado %>%
              rename(SiteID=sites),by='SiteID') %>%
  ggplot(.,aes(x=year,y=annual_mean,color=basin)) + 
  geom_point() + 
  facet_wrap(~parameter,scales='free') + 
  theme_few() + 
  scale_color_few() + 
  theme(legend.position=c(.7,.15)) + 
  guides(color=guide_legend(ncol=2))

```

## Watershed size

- Many prior publications have shown that increasing watershed size means decreasing variance in anion and cation concentrations.
- We can use our dataset to test this in the colorado basin. 

```{r}
conc.annual %>%
  left_join(site.info,by='SiteID') %>%
  filter(annual_var < 5000) %>%
  ggplot(.,aes(x=year,y=annual_var,color=area)) + 
  geom_point() + 
  facet_wrap(~parameter,scales='free') + 
  theme_few() + 
  theme(legend.position=c(.7,.15)) 
```


## Reshaping the data

- From basic weathering geochemistry principles we know that [Bicarbonate] ~  Mg + Ca
- The current shape of the data in a 'long' format makes looking at these correlations impossible.
- We need to 'widen' the data so the constituents are arranged in sideXside columns. 
- This data rearranging is really easy with tidyr `spread` and `gather`

```{r}
conc.wide <- conc.annual %>%
  select(-annual_var) %>%
  spread(key=parameter,value=annual_mean) %>%
  mutate(`Mg+Ca`=Magnesium+Calcium)

head(conc.wide)

```

## Plot of Bicarbonate vs Mg+Ca by site

```{r}

ggplot(conc.wide,aes(x=Bicarbonate,y=`Mg+Ca`,color=SiteID)) + 
  geom_point() + 
  geom_abline(slope=1,intercept=0)


```


# Model changes

<div class="notes">
It looks to me like there might be some trends in the data at certain sites. (Mg and SO4 in particular). Let's use some advanced r to check if there are some linear trends in these datasets. 
</div>

## Nesting and modelling


- `purrr` and `tidyr` make doing multiple models on different sites really easy. 
- "Nesting" data wraps data into a nice tidy bundle for parallel analyses down the road
- These "nests" are *inside* of the tibble. Making it easy to look at them. Easier than lists anyhow!

```{r}

conc.nest <- conc.annual %>%
  group_by(parameter,SiteID) %>%
  nest() 

head(conc.nest)
```

## Modelling on nested data using `purrr`

- We can then build a simple model (concentration ~ year)
- This model can be applied to all sites using a `purrr::map` function


```{r}
#Create a generic model function (mean as afunction of time)
time.lm.models <- function(x){
  mod <- lm(annual_mean~year,data=x)
}

conc.models <- conc.nest %>%
  mutate(mods=map(data,time.lm.models))

head(conc.models)

```

## Using `broom` to look at model results

- We now have a series of nested model results
- But we want to look at those summary results
- `broom::glance` makes looking at these data clean and tidy

```{r}
conc.models %>%
  mutate(mod.glance=map(mods,glance)) %>%
  unnest(mod.glance) %>% #Unnesting unwraps the nested column. 
  arrange(desc(adj.r.squared)) %>%
  select(parameter,SiteID,adj.r.squared,p.value,logLik,AIC) %>%
  kable(.,'html') %>%
  kable_styling() %>%
  scroll_box(width='600px',height='500px')


```



# Fin
